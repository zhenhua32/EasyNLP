{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import easynlp\n",
    "from easynlp.appzoo import FewshotSequenceClassificationDataset\n",
    "# from easynlp.fewshot_learning import FewshotBaseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(r\"G:\\code\\pretrain_model_dir\\bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 143, 704, 1744, 100, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"a中国A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'a', '中', '国', '[UNK]', '[SEP]']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([101, 143, 704, 1744, 100, 102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****./tmp/fewshot_data/train.csv\n"
     ]
    }
   ],
   "source": [
    "dataset = FewshotSequenceClassificationDataset(\n",
    "    pretrained_model_name_or_path= r\"G:\\code\\pretrain_model_dir\\pai-bert-base-zh\",\n",
    "    data_file=\"./tmp/fewshot_data/train.csv\",\n",
    "    max_seq_length=32,\n",
    "    first_sequence=\"text\",\n",
    "    input_schema=\"text:str:1,label:str:1\",\n",
    "    label_name=\"label\",\n",
    "    label_enumerate_values=\"Positive,Negative\",\n",
    "    user_defined_parameters={\n",
    "        \"app_parameters\": {\n",
    "            \"label_desc\": \"好的,差的\",\n",
    "            \"pattern\": \"text,是一条商品,label,评\",\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'token_type_ids', 'label_ids', 'mask_span_indices'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=dataset.batch_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([2, 32])\n",
      "attention_mask torch.Size([2, 32])\n",
      "token_type_ids torch.Size([2, 32])\n",
      "label_ids torch.Size([2, 32])\n",
      "mask_span_indices torch.Size([2, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "inputs = next(iter(loader))\n",
    "\n",
    "for key, val in inputs.items():\n",
    "    print(key, val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[28],\n",
       "          [29]],\n",
       " \n",
       "         [[28],\n",
       "          [29]]]),\n",
       " torch.Size([2, 2, 1]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"mask_span_indices\"], inputs[\"mask_span_indices\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, 1962, -100, -100],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, 1962, -100, -100]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"label_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'好'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(1962)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
